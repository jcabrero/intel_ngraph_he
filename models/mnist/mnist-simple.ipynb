{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/he-transformer/build/ext_ngraph_tf/src/ext_ngraph_tf/build_cmake/venv-tf-py3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "# Tensorflow and keras layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense,\\\n",
    "                                    Activation, ZeroPadding2D,\\\n",
    "                                    BatchNormalization, Flatten, Conv2D, Conv2DTranspose, Reshape\n",
    "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D,\\\n",
    "                                    Dropout, GlobalMaxPooling2D,\\\n",
    "                                    GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "# To generate GIFs\n",
    "\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import ngraph_bridge\n",
    "\n",
    "# For loading and making use of HE Transformer\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  models  already exists\n"
     ]
    }
   ],
   "source": [
    "def create_mnist_npy():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    X = np.concatenate([x_train, x_test]).reshape(-1, 28*28)\n",
    "    Y = np.concatenate([y_train, y_test]).reshape(-1, 1)\n",
    "    #print(X.shape, Y.shape)\n",
    "    T = np.concatenate([X, Y], axis=1)\n",
    "    np.save(\"mnist.npy\", T)\n",
    "\n",
    "def load_mnist():\n",
    "    T = np.load('mnist.npy')\n",
    "    X = T[:, :-1].reshape(-1, 28, 28)\n",
    "    Y = T[:, -1]\n",
    "    x_train, x_test = X[:-10000], X[-10000:]\n",
    "    y_train, y_test = Y[:-10000], Y[-10000:]\n",
    "    y_train = tf.compat.v1.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test = tf.compat.v1.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "    #print(y_train.shape, y_test.shape, y_train)\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "def create_dir(dirname):\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(dirname)\n",
    "        #print(\"dirname \" , filename ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , dirname ,  \" already exists\")\n",
    "\n",
    "create_dir('models')\n",
    "\n",
    "def plot_4_by_4_images(x, save = False, savefile=\"img.png\"):\n",
    "    w=10\n",
    "    h=10\n",
    "    fig=plt.figure(figsize=(w, h))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = np.random.randint(x.shape[0])\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(x[i - 1, :, :, 0], cmap='gray')\n",
    "    if save:\n",
    "        plt.savefig(savefile)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    \n",
    "def generate_noise_samples(n=1):\n",
    "    noise = np.random.normal(0, 1, (n, 100))\n",
    "    return noise\n",
    "\n",
    "\n",
    "def generate_fake_samples(generator, n=1):\n",
    "    noise = generate_noise_samples(n)\n",
    "    X = generator.predict(noise)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_and_save_images(generator, noise_test, epoch, filename):\n",
    "    display.clear_output(wait=True)\n",
    "    fake_images = generator.predict(noise_test)\n",
    "    plot_4_by_4_images(fake_images, save=True, savefile='models/{}/img/{:04d}.png'.format(filename, epoch))\n",
    "\n",
    "    \n",
    "def train_step(generator, discriminator, gan, real_images, batch_size=64):\n",
    "    real_label = np.ones((batch_size, 1))\n",
    "    generated_images = generate_fake_samples(generator, batch_size)\n",
    "    generated_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "    X_dis = np.concatenate([real_images, generated_images])\n",
    "    y_dis = np.zeros(2*batch_size)\n",
    "    y_dis[:batch_size]=0.9\n",
    "        \n",
    "    discriminator.trainable = True\n",
    "    discriminator.train_on_batch(X_dis, y_dis)\n",
    "    #discriminator.train_on_batch(x_fake, y_fake)\n",
    "        \n",
    "    discriminator.trainable = False\n",
    "    x_gan = generate_noise_samples(batch_size)\n",
    "    y_gan = np.ones((batch_size, 1)) # We assume that we wanted true as answer from the discriminator\n",
    "    gan.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "def gen_csv(data_list, filename):\n",
    "    pd.DataFrame(data_list, \n",
    "                 columns =['Epoch', 'Time (s)']).to_csv ('models/%s/times.csv'%(filename), index = False, header=True)\n",
    "\n",
    "def train(generator, discriminator, gan, dataset, epochs=50, batch_size=64, filename=str(int(time.time()))):\n",
    "    m = dataset.shape[0]\n",
    "    m_batch = m // batch_size\n",
    "    noise_test = np.random.normal(0,1, [20, 100])\n",
    "    toc = time.time()\n",
    "    time_data = []\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        for batch_num in range(m_batch):\n",
    "            tuc = time.time()\n",
    "            #if batch_num % 30 == 0:\n",
    "                #print ('[{}%] Time for epoch {} is {} sec'.format((batch_num / m_batch) * 100,epoch + 1, time.time()-tic), end='\\r')\n",
    "                #generate_and_save_images(noise_test, epoch, batch_num)\n",
    "                #print(\"[%0.2f%%] Time for epoch %d is %f sec\" % ( (batch_num / m_batch) * 100, epoch + 1, time.time()-tic), end='\\r')\n",
    "            \n",
    "            batch_slot = batch_size * batch_num\n",
    "            batch = dataset[batch_slot: batch_slot + batch_size]\n",
    "            train_step(generator, discriminator, gan, batch, batch_size)\n",
    "        time_data.append((epoch + 1, time.time()-tic))\n",
    "        generate_and_save_images(generator, noise_test, epoch, filename)\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time_data[-1][1]))\n",
    "    \n",
    "    gen_csv(time_data, filename)\n",
    "    generator.save('models/%s/generator.h5' % (filename))\n",
    "    discriminator.save('models/%s/discriminator.h5' % (filename))\n",
    "    gan.save('models/%s/gan.h5' % (filename))\n",
    "\n",
    "    \n",
    "def make_gif(anim_file, file_regex):\n",
    "    #anim_file = 'dcgan.gif'\n",
    "    fname = None\n",
    "    with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "        filenames = glob.glob(file_regex)\n",
    "        filenames = sorted(filenames)\n",
    "        last = -1\n",
    "        for i,filename in enumerate(filenames):\n",
    "            frame = 2*(i**0.75)\n",
    "            if round(frame) > round(last):\n",
    "                last = frame\n",
    "            else:\n",
    "                continue\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "        \n",
    "def format_filename(filename):\n",
    "    date = datetime.now()\n",
    "    date_str = \"_%02d_%02d_%02d_%02d_%02d\" % (date.day, date.month, date.year, date.hour, date.minute)\n",
    "    create_dir('models/'+ filename)\n",
    "    create_dir('models/'+ filename +\"/img\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nodes(graph_def=None):\n",
    "    \"\"\"Prints the node names of a graph_def.\n",
    "        If graph_def is not provided, use default graph_def\"\"\"\n",
    "\n",
    "    if graph_def is None:\n",
    "        nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    else:\n",
    "        nodes = [n.name for n in graph_def.node]\n",
    "\n",
    "    print(\"nodes\", nodes)\n",
    "\n",
    "# https://www.dlology.com/blog/how-to-convert-trained-keras-model-to-tensorflow-and-make-prediction/\n",
    "def freeze_session(session,\n",
    "                   keep_var_names=None,\n",
    "                   output_names=None,\n",
    "                   clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import (\n",
    "        convert_variables_to_constants,\n",
    "        remove_training_nodes,\n",
    "    )\n",
    "\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(\n",
    "            set(v.op.name for v in tf.global_variables()).difference(\n",
    "                keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        print_nodes(input_graph_def)\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        frozen_graph = remove_training_nodes(frozen_graph)\n",
    "        return frozen_graph\n",
    "\n",
    "def save_model(sess, output_names, directory, filename):\n",
    "    frozen_graph = freeze_session(sess, output_names=output_names)\n",
    "    print_nodes(frozen_graph)\n",
    "    tf.io.write_graph(frozen_graph, directory, filename + \".pb\", as_text=False)\n",
    "    print(\"Model saved to: %s\" % filename + \".pb\")\n",
    "\n",
    "    \n",
    "def load_pb_file(filename):\n",
    "    \"\"\"\"Returns the graph_def from a saved protobuf file\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        raise Exception(\"File, \" + filename + \" does not exist\")\n",
    "\n",
    "    with tf.io.gfile.GFile(filename, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    print(\"Model restored\")\n",
    "    return graph_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train, x_test, y_test) = load_mnist()\n",
    "plot_4_by_4_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The cell below includes the cryptonets model. It is composed of the following:\n",
    "1. A convolutional layer , with 5 filters of size 5x5, same padding and stride 2. $n^{[l]} = \\dfrac{(n^{[l-1]} + 2 * p^{[l]} - f^{[l]})}{s^{[l]}} + 1$.\n",
    " - Input: 28 x 28 x 1\n",
    " - Output: 14 x 14 x 5\n",
    "2. A square activation function $g(x) = x^2$\n",
    "3. An average pooling layer with 1 stride and same padding. $n^{[l]} = \\dfrac{(n^{[l-1]} - f^{[l]})}{s^{[l]}} + 1$.\n",
    " - Input: 14 x 14 x 5\n",
    " - Output: 14 x 14 x5\n",
    "4. Another convolutional layer , with 50 filters of size 5x5, same padding and stride 2. $n^{[l]} = \\dfrac{(n^{[l-1]} + 2 * p^{[l]} - f^{[l]})}{s^{[l]}} + 1$.\n",
    " - Input: 14 x 14 x 5\n",
    " - Output: 7 x 7 x 50\n",
    "5. Another average pooling layer.\n",
    " - Input: 7 x 7 x 50\n",
    " - Output: 7 x 7 x 50\n",
    "6. A flattening layer, thus resulting in 7 x 7 x 50 = 2450 input attributes to the dense layer\n",
    "7. A dense layer whose output is 100 from 2450\n",
    "8. Another square activation layer\n",
    "9. A final dense layer without any activation after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_mlp_model(input):\n",
    "    y = Conv2D(\n",
    "        filters=5,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=True,\n",
    "        input_shape=(28, 28, 1),\n",
    "    )(input)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    y = MaxPooling2D(pool_size=(3, 3))(y)\n",
    "\n",
    "    y = Conv2D(\n",
    "        filters=50,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=True,\n",
    "    )(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    known_shape = y.get_shape()[1:]\n",
    "    size = np.prod(known_shape)\n",
    "    print('size', size)\n",
    "\n",
    "    # Using Keras model API with Flatten results in split ngraph at Flatten() or Reshape() op.\n",
    "    # Use tf.reshape instead\n",
    "    y = tf.reshape(y, [-1, size])\n",
    "\n",
    "    y = Dense(100, use_bias=True)(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "    y = Dense(10, use_bias=True, name=\"output\")(y)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "The model is created in a \"tensorflow-like\" way. We use the Keras layers to pass through them a placeholder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 200\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 5)         130       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 50)          6300      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2, 2, 50)          0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorF [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 27,540\n",
      "Trainable params: 27,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(28, 28, 1), name=\"input\")\n",
    "y = mnist_mlp_model(x)\n",
    "mnist_mlp = Model(inputs=x, outputs=y)\n",
    "print(mnist_mlp.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We first define the loss and the optimizer that we are going to make use of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "optimizer = SGD(learning_rate=0.008, momentum=0.9)\n",
    "mnist_mlp.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.4691 - acc: 0.8578 - val_loss: 0.1415 - val_acc: 0.9562\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.1343 - acc: 0.9579 - val_loss: 0.1179 - val_acc: 0.9617\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.1076 - acc: 0.9662 - val_loss: 0.0912 - val_acc: 0.9707\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0936 - acc: 0.9705 - val_loss: 0.0815 - val_acc: 0.9727\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0825 - acc: 0.9746 - val_loss: 0.0772 - val_acc: 0.9760\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.0756 - acc: 0.9757 - val_loss: 0.0767 - val_acc: 0.9765\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0692 - acc: 0.9783 - val_loss: 0.0694 - val_acc: 0.9780\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0639 - acc: 0.9795 - val_loss: 0.0686 - val_acc: 0.9781\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0578 - acc: 0.9824 - val_loss: 0.0713 - val_acc: 0.9781\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0542 - acc: 0.9827 - val_loss: 0.0691 - val_acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f91b359eb00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_mlp.fit(x_train, y_train, \n",
    "                     epochs=10, batch_size=64, \n",
    "                     validation_data=(x_test, y_test), \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mnist_mlp.save('models/mnist_mlp.h5')\n",
    "#tf.saved_model.save(mnist_mlp, 'models/mnist_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 98us/sample - loss: 0.0691 - acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = mnist_mlp.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes ['input', 'conv2d/kernel/Initializer/random_uniform/shape', 'conv2d/kernel/Initializer/random_uniform/min', 'conv2d/kernel/Initializer/random_uniform/max', 'conv2d/kernel/Initializer/random_uniform/RandomUniform', 'conv2d/kernel/Initializer/random_uniform/sub', 'conv2d/kernel/Initializer/random_uniform/mul', 'conv2d/kernel/Initializer/random_uniform', 'conv2d/kernel', 'conv2d/kernel/IsInitialized/VarIsInitializedOp', 'conv2d/kernel/Assign', 'conv2d/kernel/Read/ReadVariableOp', 'conv2d/bias/Initializer/zeros', 'conv2d/bias', 'conv2d/bias/IsInitialized/VarIsInitializedOp', 'conv2d/bias/Assign', 'conv2d/bias/Read/ReadVariableOp', 'conv2d/dilation_rate', 'conv2d/Conv2D/ReadVariableOp', 'conv2d/Conv2D', 'conv2d/BiasAdd/ReadVariableOp', 'conv2d/BiasAdd', 'activation/Relu', 'max_pooling2d/MaxPool', 'conv2d_1/kernel/Initializer/random_uniform/shape', 'conv2d_1/kernel/Initializer/random_uniform/min', 'conv2d_1/kernel/Initializer/random_uniform/max', 'conv2d_1/kernel/Initializer/random_uniform/RandomUniform', 'conv2d_1/kernel/Initializer/random_uniform/sub', 'conv2d_1/kernel/Initializer/random_uniform/mul', 'conv2d_1/kernel/Initializer/random_uniform', 'conv2d_1/kernel', 'conv2d_1/kernel/IsInitialized/VarIsInitializedOp', 'conv2d_1/kernel/Assign', 'conv2d_1/kernel/Read/ReadVariableOp', 'conv2d_1/bias/Initializer/zeros', 'conv2d_1/bias', 'conv2d_1/bias/IsInitialized/VarIsInitializedOp', 'conv2d_1/bias/Assign', 'conv2d_1/bias/Read/ReadVariableOp', 'conv2d_1/dilation_rate', 'conv2d_1/Conv2D/ReadVariableOp', 'conv2d_1/Conv2D', 'conv2d_1/BiasAdd/ReadVariableOp', 'conv2d_1/BiasAdd', 'activation_1/Relu', 'Reshape/shape', 'Reshape', 'group_deps', 'VarIsInitializedOp', 'VarIsInitializedOp_1', 'VarIsInitializedOp_2', 'VarIsInitializedOp_3', 'init', 'dense/kernel/Initializer/random_uniform/shape', 'dense/kernel/Initializer/random_uniform/min', 'dense/kernel/Initializer/random_uniform/max', 'dense/kernel/Initializer/random_uniform/RandomUniform', 'dense/kernel/Initializer/random_uniform/sub', 'dense/kernel/Initializer/random_uniform/mul', 'dense/kernel/Initializer/random_uniform', 'dense/kernel', 'dense/kernel/IsInitialized/VarIsInitializedOp', 'dense/kernel/Assign', 'dense/kernel/Read/ReadVariableOp', 'dense/bias/Initializer/zeros', 'dense/bias', 'dense/bias/IsInitialized/VarIsInitializedOp', 'dense/bias/Assign', 'dense/bias/Read/ReadVariableOp', 'dense/MatMul/ReadVariableOp', 'dense/MatMul', 'dense/BiasAdd/ReadVariableOp', 'dense/BiasAdd', 'activation_2/Relu', 'output/kernel/Initializer/random_uniform/shape', 'output/kernel/Initializer/random_uniform/min', 'output/kernel/Initializer/random_uniform/max', 'output/kernel/Initializer/random_uniform/RandomUniform', 'output/kernel/Initializer/random_uniform/sub', 'output/kernel/Initializer/random_uniform/mul', 'output/kernel/Initializer/random_uniform', 'output/kernel', 'output/kernel/IsInitialized/VarIsInitializedOp', 'output/kernel/Assign', 'output/kernel/Read/ReadVariableOp', 'output/bias/Initializer/zeros', 'output/bias', 'output/bias/IsInitialized/VarIsInitializedOp', 'output/bias/Assign', 'output/bias/Read/ReadVariableOp', 'output/MatMul/ReadVariableOp', 'output/MatMul', 'output/BiasAdd/ReadVariableOp', 'output/BiasAdd', 'output_target', 'total/Initializer/zeros', 'total', 'total/IsInitialized/VarIsInitializedOp', 'total/Assign', 'total/Read/ReadVariableOp', 'count/Initializer/zeros', 'count', 'count/IsInitialized/VarIsInitializedOp', 'count/Assign', 'count/Read/ReadVariableOp', 'metrics/acc/ArgMax/dimension', 'metrics/acc/ArgMax', 'metrics/acc/ArgMax_1/dimension', 'metrics/acc/ArgMax_1', 'metrics/acc/Equal', 'metrics/acc/Cast', 'metrics/acc/Const', 'metrics/acc/Sum', 'metrics/acc/AssignAddVariableOp', 'metrics/acc/ReadVariableOp', 'metrics/acc/Size', 'metrics/acc/Cast_1', 'metrics/acc/AssignAddVariableOp_1', 'metrics/acc/ReadVariableOp_1', 'metrics/acc/div_no_nan/ReadVariableOp', 'metrics/acc/div_no_nan/ReadVariableOp_1', 'metrics/acc/div_no_nan', 'metrics/acc/Identity', 'loss/output_loss/Const', 'loss/output_loss/softmax_cross_entropy_with_logits/Rank', 'loss/output_loss/softmax_cross_entropy_with_logits/Shape', 'loss/output_loss/softmax_cross_entropy_with_logits/Rank_1', 'loss/output_loss/softmax_cross_entropy_with_logits/Shape_1', 'loss/output_loss/softmax_cross_entropy_with_logits/Sub/y', 'loss/output_loss/softmax_cross_entropy_with_logits/Sub', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice/begin', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice/size', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice', 'loss/output_loss/softmax_cross_entropy_with_logits/concat/values_0', 'loss/output_loss/softmax_cross_entropy_with_logits/concat/axis', 'loss/output_loss/softmax_cross_entropy_with_logits/concat', 'loss/output_loss/softmax_cross_entropy_with_logits/Reshape', 'loss/output_loss/softmax_cross_entropy_with_logits/Rank_2', 'loss/output_loss/softmax_cross_entropy_with_logits/Shape_2', 'loss/output_loss/softmax_cross_entropy_with_logits/Sub_1/y', 'loss/output_loss/softmax_cross_entropy_with_logits/Sub_1', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice_1/begin', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice_1/size', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice_1', 'loss/output_loss/softmax_cross_entropy_with_logits/concat_1/values_0', 'loss/output_loss/softmax_cross_entropy_with_logits/concat_1/axis', 'loss/output_loss/softmax_cross_entropy_with_logits/concat_1', 'loss/output_loss/softmax_cross_entropy_with_logits/Reshape_1', 'loss/output_loss/softmax_cross_entropy_with_logits', 'loss/output_loss/softmax_cross_entropy_with_logits/Sub_2/y', 'loss/output_loss/softmax_cross_entropy_with_logits/Sub_2', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice_2/begin', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice_2/size', 'loss/output_loss/softmax_cross_entropy_with_logits/Slice_2', 'loss/output_loss/softmax_cross_entropy_with_logits/Reshape_2', 'loss/output_loss/weighted_loss/Const', 'loss/output_loss/weighted_loss/broadcast_weights/assert_broadcastable/weights/shape', 'loss/output_loss/weighted_loss/broadcast_weights/assert_broadcastable/weights/rank', 'loss/output_loss/weighted_loss/broadcast_weights/assert_broadcastable/values/shape', 'loss/output_loss/weighted_loss/broadcast_weights/assert_broadcastable/values/rank', 'loss/output_loss/weighted_loss/broadcast_weights/assert_broadcastable/static_scalar_check_success', 'loss/output_loss/weighted_loss/broadcast_weights/ones_like/Shape', 'loss/output_loss/weighted_loss/broadcast_weights/ones_like/Const', 'loss/output_loss/weighted_loss/broadcast_weights/ones_like', 'loss/output_loss/weighted_loss/broadcast_weights', 'loss/output_loss/weighted_loss/Mul', 'loss/output_loss/Const_1', 'loss/output_loss/Sum', 'loss/output_loss/num_elements', 'loss/output_loss/num_elements/Cast', 'loss/output_loss/Const_2', 'loss/output_loss/Sum_1', 'loss/output_loss/value', 'loss/mul/x', 'loss/mul', 'keras_learning_phase/input', 'keras_learning_phase', 'SGD/gradients/Shape', 'SGD/gradients/grad_ys_0', 'SGD/gradients/Fill', 'SGD/gradients/loss/mul_grad/Mul', 'SGD/gradients/loss/mul_grad/Mul_1', 'SGD/gradients/loss/output_loss/value_grad/Shape', 'SGD/gradients/loss/output_loss/value_grad/Shape_1', 'SGD/gradients/loss/output_loss/value_grad/BroadcastGradientArgs', 'SGD/gradients/loss/output_loss/value_grad/div_no_nan', 'SGD/gradients/loss/output_loss/value_grad/Sum', 'SGD/gradients/loss/output_loss/value_grad/Reshape', 'SGD/gradients/loss/output_loss/value_grad/Neg', 'SGD/gradients/loss/output_loss/value_grad/div_no_nan_1', 'SGD/gradients/loss/output_loss/value_grad/div_no_nan_2', 'SGD/gradients/loss/output_loss/value_grad/mul', 'SGD/gradients/loss/output_loss/value_grad/Sum_1', 'SGD/gradients/loss/output_loss/value_grad/Reshape_1', 'SGD/gradients/loss/output_loss/Sum_1_grad/Reshape/shape', 'SGD/gradients/loss/output_loss/Sum_1_grad/Reshape', 'SGD/gradients/loss/output_loss/Sum_1_grad/Const', 'SGD/gradients/loss/output_loss/Sum_1_grad/Tile', 'SGD/gradients/loss/output_loss/Sum_grad/Reshape/shape', 'SGD/gradients/loss/output_loss/Sum_grad/Reshape', 'SGD/gradients/loss/output_loss/Sum_grad/Shape', 'SGD/gradients/loss/output_loss/Sum_grad/Tile', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Shape', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Shape_1', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/BroadcastGradientArgs', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Mul', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Sum', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Reshape', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Mul_1', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Sum_1', 'SGD/gradients/loss/output_loss/weighted_loss/Mul_grad/Reshape_1', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits/Reshape_2_grad/Shape', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits/Reshape_2_grad/Reshape', 'SGD/gradients/zeros_like', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/ExpandDims/dim', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/ExpandDims', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/mul', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/LogSoftmax', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/Neg', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/ExpandDims_1/dim', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/ExpandDims_1', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits_grad/mul_1', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits/Reshape_grad/Shape', 'SGD/gradients/loss/output_loss/softmax_cross_entropy_with_logits/Reshape_grad/Reshape', 'SGD/gradients/output/BiasAdd_grad/BiasAddGrad', 'SGD/gradients/output/MatMul_grad/MatMul', 'SGD/gradients/output/MatMul_grad/MatMul_1', 'SGD/gradients/activation_2/Relu_grad/ReluGrad', 'SGD/gradients/dense/BiasAdd_grad/BiasAddGrad', 'SGD/gradients/dense/MatMul_grad/MatMul', 'SGD/gradients/dense/MatMul_grad/MatMul_1', 'SGD/gradients/Reshape_grad/Shape', 'SGD/gradients/Reshape_grad/Reshape', 'SGD/gradients/activation_1/Relu_grad/ReluGrad', 'SGD/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad', 'SGD/gradients/conv2d_1/Conv2D_grad/ShapeN', 'SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput', 'SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter', 'SGD/gradients/max_pooling2d/MaxPool_grad/MaxPoolGrad', 'SGD/gradients/activation/Relu_grad/ReluGrad', 'SGD/gradients/conv2d/BiasAdd_grad/BiasAddGrad', 'SGD/gradients/conv2d/Conv2D_grad/ShapeN', 'SGD/gradients/conv2d/Conv2D_grad/Conv2DBackpropInput', 'SGD/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter', 'SGD/iter/Initializer/zeros', 'SGD/iter', 'SGD/iter/IsInitialized/VarIsInitializedOp', 'SGD/iter/Assign', 'SGD/iter/Read/ReadVariableOp', 'SGD/decay/Initializer/initial_value', 'SGD/decay', 'SGD/decay/IsInitialized/VarIsInitializedOp', 'SGD/decay/Assign', 'SGD/decay/Read/ReadVariableOp', 'SGD/learning_rate/Initializer/initial_value', 'SGD/learning_rate', 'SGD/learning_rate/IsInitialized/VarIsInitializedOp', 'SGD/learning_rate/Assign', 'SGD/learning_rate/Read/ReadVariableOp', 'SGD/momentum/Initializer/initial_value', 'SGD/momentum', 'SGD/momentum/IsInitialized/VarIsInitializedOp', 'SGD/momentum/Assign', 'SGD/momentum/Read/ReadVariableOp', 'SGD/conv2d/kernel/momentum/Initializer/zeros', 'SGD/conv2d/kernel/momentum', 'SGD/conv2d/kernel/momentum/IsInitialized/VarIsInitializedOp', 'SGD/conv2d/kernel/momentum/Assign', 'SGD/conv2d/kernel/momentum/Read/ReadVariableOp', 'SGD/conv2d/bias/momentum/Initializer/zeros', 'SGD/conv2d/bias/momentum', 'SGD/conv2d/bias/momentum/IsInitialized/VarIsInitializedOp', 'SGD/conv2d/bias/momentum/Assign', 'SGD/conv2d/bias/momentum/Read/ReadVariableOp', 'SGD/conv2d_1/kernel/momentum/Initializer/zeros/shape_as_tensor', 'SGD/conv2d_1/kernel/momentum/Initializer/zeros/Const', 'SGD/conv2d_1/kernel/momentum/Initializer/zeros', 'SGD/conv2d_1/kernel/momentum', 'SGD/conv2d_1/kernel/momentum/IsInitialized/VarIsInitializedOp', 'SGD/conv2d_1/kernel/momentum/Assign', 'SGD/conv2d_1/kernel/momentum/Read/ReadVariableOp', 'SGD/conv2d_1/bias/momentum/Initializer/zeros', 'SGD/conv2d_1/bias/momentum', 'SGD/conv2d_1/bias/momentum/IsInitialized/VarIsInitializedOp', 'SGD/conv2d_1/bias/momentum/Assign', 'SGD/conv2d_1/bias/momentum/Read/ReadVariableOp', 'SGD/dense/kernel/momentum/Initializer/zeros/shape_as_tensor', 'SGD/dense/kernel/momentum/Initializer/zeros/Const', 'SGD/dense/kernel/momentum/Initializer/zeros', 'SGD/dense/kernel/momentum', 'SGD/dense/kernel/momentum/IsInitialized/VarIsInitializedOp', 'SGD/dense/kernel/momentum/Assign', 'SGD/dense/kernel/momentum/Read/ReadVariableOp', 'SGD/dense/bias/momentum/Initializer/zeros', 'SGD/dense/bias/momentum', 'SGD/dense/bias/momentum/IsInitialized/VarIsInitializedOp', 'SGD/dense/bias/momentum/Assign', 'SGD/dense/bias/momentum/Read/ReadVariableOp', 'SGD/output/kernel/momentum/Initializer/zeros/shape_as_tensor', 'SGD/output/kernel/momentum/Initializer/zeros/Const', 'SGD/output/kernel/momentum/Initializer/zeros', 'SGD/output/kernel/momentum', 'SGD/output/kernel/momentum/IsInitialized/VarIsInitializedOp', 'SGD/output/kernel/momentum/Assign', 'SGD/output/kernel/momentum/Read/ReadVariableOp', 'SGD/output/bias/momentum/Initializer/zeros', 'SGD/output/bias/momentum', 'SGD/output/bias/momentum/IsInitialized/VarIsInitializedOp', 'SGD/output/bias/momentum/Assign', 'SGD/output/bias/momentum/Read/ReadVariableOp', 'SGD/SGD/update_conv2d/kernel/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_conv2d/kernel/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_conv2d/kernel/ResourceApplyKerasMomentum', 'SGD/SGD/update_conv2d/bias/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_conv2d/bias/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_conv2d/bias/ResourceApplyKerasMomentum', 'SGD/SGD/update_conv2d_1/kernel/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_conv2d_1/kernel/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_conv2d_1/kernel/ResourceApplyKerasMomentum', 'SGD/SGD/update_conv2d_1/bias/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_conv2d_1/bias/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_conv2d_1/bias/ResourceApplyKerasMomentum', 'SGD/SGD/update_dense/kernel/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_dense/kernel/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_dense/kernel/ResourceApplyKerasMomentum', 'SGD/SGD/update_dense/bias/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_dense/bias/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_dense/bias/ResourceApplyKerasMomentum', 'SGD/SGD/update_output/kernel/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_output/kernel/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_output/kernel/ResourceApplyKerasMomentum', 'SGD/SGD/update_output/bias/ResourceApplyKerasMomentum/ReadVariableOp', 'SGD/SGD/update_output/bias/ResourceApplyKerasMomentum/ReadVariableOp_1', 'SGD/SGD/update_output/bias/ResourceApplyKerasMomentum', 'SGD/SGD/Const', 'SGD/SGD/AssignAddVariableOp', 'SGD/SGD/ReadVariableOp', 'training_1/group_deps', 'Placeholder', 'AssignVariableOp', 'ReadVariableOp', 'Placeholder_1', 'AssignVariableOp_1', 'ReadVariableOp_1', 'VarIsInitializedOp_4', 'VarIsInitializedOp_5', 'VarIsInitializedOp_6', 'VarIsInitializedOp_7', 'VarIsInitializedOp_8', 'VarIsInitializedOp_9', 'VarIsInitializedOp_10', 'VarIsInitializedOp_11', 'VarIsInitializedOp_12', 'VarIsInitializedOp_13', 'VarIsInitializedOp_14', 'VarIsInitializedOp_15', 'VarIsInitializedOp_16', 'VarIsInitializedOp_17', 'VarIsInitializedOp_18', 'VarIsInitializedOp_19', 'VarIsInitializedOp_20', 'VarIsInitializedOp_21', 'init_1', 'evaluation/group_deps']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes ['input', 'conv2d/kernel', 'conv2d/bias', 'conv2d/Conv2D', 'conv2d/BiasAdd', 'activation/Relu', 'max_pooling2d/MaxPool', 'conv2d_1/kernel', 'conv2d_1/bias', 'conv2d_1/Conv2D', 'conv2d_1/BiasAdd', 'activation_1/Relu', 'Reshape/shape', 'Reshape', 'dense/kernel', 'dense/bias', 'dense/MatMul', 'dense/BiasAdd', 'activation_2/Relu', 'output/kernel', 'output/bias', 'output/MatMul', 'output/BiasAdd', 'SGD/iter', 'SGD/decay', 'SGD/learning_rate', 'SGD/momentum', 'SGD/conv2d/kernel/momentum', 'SGD/conv2d/bias/momentum', 'SGD/conv2d_1/kernel/momentum', 'SGD/conv2d_1/bias/momentum', 'SGD/dense/kernel/momentum', 'SGD/dense/bias/momentum', 'SGD/output/kernel/momentum', 'SGD/output/bias/momentum']\n",
      "Model saved to: mlp.pb\n"
     ]
    }
   ],
   "source": [
    "save_model(tf.compat.v1.keras.backend.get_session(), [\"output/BiasAdd\"], \"./models\", \"mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Homorphic Encryption Configuration\n",
    "We generate the parameters for the encryption. These parameteres usually set information about the Learning with Error (LWE) problem. The explanation of how to obtain the parameters is inline with the code. It is mainly extracted from the documentation of Intel nGraph HE Transformer Github Repository. After having executed this section code, a file 'config.json' has been generated with the configuration for the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scheme_name': 'HE_SEAL', 'poly_modulus_degree': 8192, 'security_level': 128, 'coeff_modulus': [30, 24, 24, 24, 24, 24, 24, 30], 'scale': 16777216, 'complex_packing': False}\n",
      "Generated configuration in config.json\n"
     ]
    }
   ],
   "source": [
    "def get_N(total_coefficient_modulus_bit_width, security_level, sec_type='classical'):\n",
    "    sec = {\n",
    "        'classical':{\n",
    "            128:{\n",
    "                1024:  27,\n",
    "                2048:  54,\n",
    "                4096:  109,\n",
    "                8192:  218,\n",
    "                16384: 438,\n",
    "                32768: 881\n",
    "            },\n",
    "            192:{\n",
    "                1024:  19,\n",
    "                2048:  37,\n",
    "                4096:  75,\n",
    "                8192:  152,\n",
    "                16384: 305,\n",
    "                32768: 611\n",
    "            },\n",
    "            256:{\n",
    "                1024:  14,\n",
    "                2048:  29,\n",
    "                4096:  58,\n",
    "                8192:  118,\n",
    "                16384: 237,\n",
    "                32768: 476\n",
    "            }\n",
    "        },\n",
    "        'quantum':{\n",
    "           128:{\n",
    "                1024:  25,\n",
    "                2048:  51,\n",
    "                4096:  101,\n",
    "                8192:  202,\n",
    "                16384: 411,\n",
    "                32768: 827\n",
    "            },\n",
    "            192:{\n",
    "                1024:  17,\n",
    "                2048:  35,\n",
    "                4096:  70,\n",
    "                8192:  141,\n",
    "                16384: 284,\n",
    "                32768: 571\n",
    "            },\n",
    "            256:{\n",
    "                1024:  13,\n",
    "                2048:  27,\n",
    "                4096:  54,\n",
    "                8192:  109,\n",
    "                16384: 220,\n",
    "                32768: 443\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    log2q_prev = 0\n",
    "    for n, log2q in sec[sec_type][security_level].items():\n",
    "        if log2q_prev < total_coefficient_modulus_bit_width < log2q:\n",
    "            return n\n",
    "        log2q_prev = log2q\n",
    "    return n\n",
    "def define_encryption_params(security_level):\n",
    "    # Select the security level\n",
    "    # Compute the multiplicative depth of the computational graph\n",
    "    L = 8 # Assumed from configuration\n",
    "    # Estimate the bit-precission s, required. According to Intel, the best tradeoff is ~24 bits\n",
    "    s = 24\n",
    "    # Choose the coeff_modulus = [s, s, s, ..., s]. A list of L coefficient moduli, each with s bits. Set the scale to s.\n",
    "    coeff_modulus = [s] * L\n",
    "    coeff_modulus[0] = 30\n",
    "    coeff_modulus[-1] = 30\n",
    "    scale = s\n",
    "    # Compute the total coefficient modulus bit width, L * s in the above parameter selection.\n",
    "    total_coefficient_modulus_bit_width = L * s\n",
    "    # Set the poly_modulus_degree to the smallest power of two, with coefficient modulus smaller than the maximum allowed.\n",
    "    # Based on the table of recommended parameters. \n",
    "    N = get_N(total_coefficient_modulus_bit_width, security_level)\n",
    "    poly_modulus_degree = N\n",
    "    # For best performance, we should choose the batch_size to max_batch_size\n",
    "    max_batch_size = poly_modulus_degree / 2\n",
    "    # We should only include the complex packing if there are polynomial activations.\n",
    "    # That is, if we make use of our own activation functions\n",
    "    complex_packing = False\n",
    "    \n",
    "    enc_params = {\n",
    "        'scheme_name': 'HE_SEAL',          # Fixed, to use the HE backend\n",
    "        'poly_modulus_degree': poly_modulus_degree,       # A power of 2 {1024, 2048, 4096, 8192, 16384}\n",
    "        'security_level': security_level,             # The security we want to ensure {0, 128, 192, 256}\n",
    "        'coeff_modulus': coeff_modulus, # A number inbetween 1 and 60\n",
    "        'scale': 2 ** s,                  # The fixed bit precission of the encoding. (log2(scale) is the number of bits)\n",
    "        'complex_packing': complex_packing,\n",
    "    }\n",
    "    print(enc_params)\n",
    "    return enc_params\n",
    "def gen_json_params_file(filename, security_level=128):\n",
    "    enc_params_dict = define_encryption_params(security_level)\n",
    "    enc_params_json = json.dumps(enc_params_dict)\n",
    "    with open(filename, 'w+') as file:\n",
    "        file.write(enc_params_json)\n",
    "    print(\"Generated configuration in %s\" %(filename))\n",
    "    \n",
    "gen_json_params_file('config.json', security_level=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cell below we generate a configuration for the session. This configuration establishes, among other things that the execution is carried out by the \"HE_SEAL\" backend. We also establish where 'config.json' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"python test.py --batch_size=100 \\\n",
    "               --backend=HE_SEAL \\\n",
    "               --model_file=models/cryptonets.pb \\\n",
    "               --encrypt_server_data=true \\\n",
    "               --encryption_parameters=$HE_TRANSFORMER/configs/he_seal_ckks_config_N13_L8.json\n",
    "\"\"\"\n",
    "def get_config_for_ngraph_server(tensor_param_name):\n",
    "    rewriter_options = rewriter_config_pb2.RewriterConfig()\n",
    "    rewriter_options.meta_optimizer_iterations = rewriter_config_pb2.RewriterConfig.ONE\n",
    "    rewriter_options.min_graph_nodes = -1\n",
    "    server_config = rewriter_options.custom_optimizers.add()\n",
    "    server_config.name = \"ngraph-optimizer\"\n",
    "    server_config.parameter_map[\"ngraph_backend\"].s = b'HE_SEAL'\n",
    "    server_config.parameter_map[\"device_id\"].s = b\"\"\n",
    "    server_config.parameter_map[\"encryption_parameters\"].s = b'config.json'\n",
    "    server_config.parameter_map[\"enable_client\"].s = b'false'\n",
    "    # Only server\n",
    "    server_config.parameter_map[tensor_param_name].s = b\"encrypt\"\n",
    "    # With client\n",
    "    #server_config.parameter_map[tensor_param_name].s = b\"client_input\"\n",
    "    # Pack data\n",
    "    server_config.parameter_map[tensor_param_name].s += b\",packed\"\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.MergeFrom(\n",
    "            tf.compat.v1.ConfigProto(\n",
    "                graph_options=tf.compat.v1.GraphOptions(\n",
    "                    rewrite_options=rewriter_options)))\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__NOTE__: It is not possible to encode more than __4096 samples__ at the same time. Thus, we only include up to that number. If we put more training samples, we might encounter problems.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "(x_train, y_train, x_test, y_test) = load_mnist()\n",
    "x_test = x_test[:4096]\n",
    "y_test = y_test[:4096]\n",
    "tf.import_graph_def(load_pb_file('./models/cryptonets.pb'))\n",
    "# Get input / output tensors\n",
    "x_input = tf.compat.v1.get_default_graph().get_tensor_by_name('import/input:0')\n",
    "y_output = tf.compat.v1.get_default_graph().get_tensor_by_name('import/output/BiasAdd:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code on the cell below, runs on homomorphically encrypted data.\n",
    "\n",
    "The code is using the HE Transformer backend, and runs on homomorphically encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Test Size (4096, 28, 28, 1)\n",
      "Total time(s) 7.999\n",
      "Error count 81 of elements.\n",
      "Accuracy: 0.980225 \n"
     ]
    }
   ],
   "source": [
    "config = get_config_for_ngraph_server(x_input.name)\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    print(\"X Test Size\", x_test.shape)\n",
    "    y_hat = y_output.eval(feed_dict={x_input: x_test})\n",
    "    elasped_time = time.time() - start_time\n",
    "    print(\"Total time(s)\", np.round(elasped_time, 3))\n",
    "    y_test_label = np.argmax(y_test, 1)\n",
    "    #print(\"y_hat\", np.round(y_hat, 2))\n",
    "    y_pred = np.argmax(y_hat, 1)\n",
    "    correct_prediction = np.equal(y_pred, y_test_label)\n",
    "    error_count = np.size(correct_prediction) - np.sum(correct_prediction)\n",
    "    test_accuracy = np.mean(correct_prediction)\n",
    "\n",
    "    print(\"Error count\", error_count, \"of elements.\")\n",
    "    print(\"Accuracy: %g \" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the client-server example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python test.py --backend=HE_SEAL \\\n",
    "               --model_file=models/cryptonets.pb \\\n",
    "               --enable_client=true \\\n",
    "               --encryption_parameters=$HE_TRANSFORMER/configs/he_seal_ckks_config_N13_L8.json\n",
    "\"\"\"\n",
    "def get_config_for_ngraph_client(tensor_param_name):\n",
    "    rewriter_options = rewriter_config_pb2.RewriterConfig()\n",
    "    rewriter_options.meta_optimizer_iterations = rewriter_config_pb2.RewriterConfig.ONE\n",
    "    rewriter_options.min_graph_nodes = -1\n",
    "    server_config = rewriter_options.custom_optimizers.add()\n",
    "    server_config.name = \"ngraph-optimizer\"\n",
    "    server_config.parameter_map[\"ngraph_backend\"].s = b'HE_SEAL'\n",
    "    server_config.parameter_map[\"device_id\"].s = b\"\"\n",
    "    server_config.parameter_map[\"encryption_parameters\"].s = b'config.json'\n",
    "    server_config.parameter_map[\"enable_client\"].s = str(True).encode()\n",
    "    \n",
    "    server_config.parameter_map[\"enable_gc\"].s = b'False'\n",
    "    server_config.parameter_map[\"mask_gc_inputs\"].s = b'False'\n",
    "    server_config.parameter_map[\"mask_gc_outputs\"].s = b'False'\n",
    "    server_config.parameter_map[\"num_gc_threads\"].s = b'1'\n",
    "    # Only server\n",
    "    # server_config.parameter_map[tensor_param_name].s = b\"encrypt\"\n",
    "    # With client\n",
    "    server_config.parameter_map[tensor_param_name].s = b\"client_input\"\n",
    "    # Pack data\n",
    "    server_config.parameter_map[tensor_param_name].s += b\",packed\"\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.MergeFrom(\n",
    "            tf.compat.v1.ConfigProto(\n",
    "                graph_options=tf.compat.v1.GraphOptions(\n",
    "                    rewrite_options=rewriter_options)))\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n",
      "total time(s) 0.045\n"
     ]
    }
   ],
   "source": [
    "# %%px --target [1] --noblock\n",
    "tf.reset_default_graph()\n",
    "tf.import_graph_def(load_pb_file('./models/mlp.pb'))\n",
    "# Get input / output tensors\n",
    "x_input = tf.compat.v1.get_default_graph().get_tensor_by_name('import/input:0')\n",
    "y_output = tf.compat.v1.get_default_graph().get_tensor_by_name('import/output/BiasAdd:0')\n",
    "\n",
    "# Create configuration to encrypt input\n",
    "config = get_config_for_ngraph_client(x_input.name)\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    y_hat = y_output.eval(feed_dict={x_input: x_test})\n",
    "    elasped_time = time.time() - start_time\n",
    "    print(\"total time(s)\", np.round(elasped_time, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c0799917ddac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mencrypt_data_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencrypt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# To break the execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "python pyclient_mnist.py --batch_size=1024 \\\n",
    "                         --encrypt_data_str=encrypt\n",
    "\"\"\"\n",
    "1 / 0 # To break the execution\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pyhe_client\n",
    "\n",
    "batch_size = 1024\n",
    "(x_train, y_train, x_test, y_test) = load_mnist()\n",
    "data = x_test[:batch_size].flatten(\"C\")\n",
    "print(\"A\")\n",
    "client = pyhe_client.HESealClient('localhost', 34000, batch_size,\n",
    "                                  {'import/input': (\"encrypt\", data)})\n",
    "\n",
    "print(\"B\")\n",
    "results = np.round(client.get_results(), 2)\n",
    "y_pred_reshape = np.array(results).reshape(batch_size, 10)\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(y_pred_reshape)\n",
    "\n",
    "y_pred = y_pred_reshape.argmax(axis=1)\n",
    "print(\"y_pred\", y_pred)\n",
    "\n",
    "correct = np.sum(np.equal(y_pred, y_test.argmax(axis=1)))\n",
    "acc = correct / float(FLAGS.batch_size)\n",
    "print(\"correct\", correct)\n",
    "print(\"Accuracy (batch size\", FLAGS.batch_size, \") =\", acc * 100.0, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
