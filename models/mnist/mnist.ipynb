{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "# Tensorflow and keras layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense,\\\n",
    "                                    Activation, ZeroPadding2D,\\\n",
    "                                    BatchNormalization, Flatten, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D,\\\n",
    "                                    Dropout, GlobalMaxPooling2D,\\\n",
    "                                    GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "# To generate GIFs\n",
    "\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  models  already exists\n"
     ]
    }
   ],
   "source": [
    "def create_mnist_npy():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    X = np.concatenate([x_train, x_test]).reshape(-1, 28*28)\n",
    "    Y = np.concatenate([y_train, y_test]).reshape(-1, 1)\n",
    "    #print(X.shape, Y.shape)\n",
    "    T = np.concatenate([X, Y], axis=1)\n",
    "    np.save(\"mnist.npy\", T)\n",
    "\n",
    "def load_mnist():\n",
    "    T = np.load('mnist.npy')\n",
    "    X = T[:, :-1].reshape(-1, 28, 28)\n",
    "    Y = T[:, -1]\n",
    "    x_train, x_test = X[:-10000], X[-10000:]\n",
    "    y_train, y_test = Y[:-10000], Y[-10000:]\n",
    "    y_train = tf.compat.v1.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test = tf.compat.v1.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "    #print(y_train.shape, y_test.shape, y_train)\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "def create_dir(dirname):\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(dirname)\n",
    "        #print(\"dirname \" , filename ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , dirname ,  \" already exists\")\n",
    "\n",
    "create_dir('models')\n",
    "\n",
    "def plot_4_by_4_images(x, save = False, savefile=\"img.png\"):\n",
    "    w=10\n",
    "    h=10\n",
    "    fig=plt.figure(figsize=(w, h))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = np.random.randint(x.shape[0])\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(x[i - 1, :, :, 0], cmap='gray')\n",
    "    if save:\n",
    "        plt.savefig(savefile)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    \n",
    "def generate_noise_samples(n=1):\n",
    "    noise = np.random.normal(0, 1, (n, 100))\n",
    "    return noise\n",
    "\n",
    "\n",
    "def generate_fake_samples(generator, n=1):\n",
    "    noise = generate_noise_samples(n)\n",
    "    X = generator.predict(noise)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_and_save_images(generator, noise_test, epoch, filename):\n",
    "    display.clear_output(wait=True)\n",
    "    fake_images = generator.predict(noise_test)\n",
    "    plot_4_by_4_images(fake_images, save=True, savefile='models/{}/img/{:04d}.png'.format(filename, epoch))\n",
    "\n",
    "    \n",
    "def train_step(generator, discriminator, gan, real_images, batch_size=64):\n",
    "    real_label = np.ones((batch_size, 1))\n",
    "    generated_images = generate_fake_samples(generator, batch_size)\n",
    "    generated_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "    X_dis = np.concatenate([real_images, generated_images])\n",
    "    y_dis = np.zeros(2*batch_size)\n",
    "    y_dis[:batch_size]=0.9\n",
    "        \n",
    "    discriminator.trainable = True\n",
    "    discriminator.train_on_batch(X_dis, y_dis)\n",
    "    #discriminator.train_on_batch(x_fake, y_fake)\n",
    "        \n",
    "    discriminator.trainable = False\n",
    "    x_gan = generate_noise_samples(batch_size)\n",
    "    y_gan = np.ones((batch_size, 1)) # We assume that we wanted true as answer from the discriminator\n",
    "    gan.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "def gen_csv(data_list, filename):\n",
    "    pd.DataFrame(data_list, \n",
    "                 columns =['Epoch', 'Time (s)']).to_csv ('models/%s/times.csv'%(filename), index = False, header=True)\n",
    "\n",
    "def train(generator, discriminator, gan, dataset, epochs=50, batch_size=64, filename=str(int(time.time()))):\n",
    "    m = dataset.shape[0]\n",
    "    m_batch = m // batch_size\n",
    "    noise_test = np.random.normal(0,1, [20, 100])\n",
    "    toc = time.time()\n",
    "    time_data = []\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        for batch_num in range(m_batch):\n",
    "            tuc = time.time()\n",
    "            #if batch_num % 30 == 0:\n",
    "                #print ('[{}%] Time for epoch {} is {} sec'.format((batch_num / m_batch) * 100,epoch + 1, time.time()-tic), end='\\r')\n",
    "                #generate_and_save_images(noise_test, epoch, batch_num)\n",
    "                #print(\"[%0.2f%%] Time for epoch %d is %f sec\" % ( (batch_num / m_batch) * 100, epoch + 1, time.time()-tic), end='\\r')\n",
    "            \n",
    "            batch_slot = batch_size * batch_num\n",
    "            batch = dataset[batch_slot: batch_slot + batch_size]\n",
    "            train_step(generator, discriminator, gan, batch, batch_size)\n",
    "        time_data.append((epoch + 1, time.time()-tic))\n",
    "        generate_and_save_images(generator, noise_test, epoch, filename)\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time_data[-1][1]))\n",
    "    \n",
    "    gen_csv(time_data, filename)\n",
    "    generator.save('models/%s/generator.h5' % (filename))\n",
    "    discriminator.save('models/%s/discriminator.h5' % (filename))\n",
    "    gan.save('models/%s/gan.h5' % (filename))\n",
    "\n",
    "    \n",
    "def make_gif(anim_file, file_regex):\n",
    "    #anim_file = 'dcgan.gif'\n",
    "    fname = None\n",
    "    with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "        filenames = glob.glob(file_regex)\n",
    "        filenames = sorted(filenames)\n",
    "        last = -1\n",
    "        for i,filename in enumerate(filenames):\n",
    "            frame = 2*(i**0.75)\n",
    "            if round(frame) > round(last):\n",
    "                last = frame\n",
    "            else:\n",
    "                continue\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "        \n",
    "def format_filename(filename):\n",
    "    date = datetime.now()\n",
    "    date_str = \"_%02d_%02d_%02d_%02d_%02d\" % (date.day, date.month, date.year, date.hour, date.minute)\n",
    "    create_dir('models/'+ filename)\n",
    "    create_dir('models/'+ filename +\"/img\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nodes(graph_def=None):\n",
    "    \"\"\"Prints the node names of a graph_def.\n",
    "        If graph_def is not provided, use default graph_def\"\"\"\n",
    "\n",
    "    if graph_def is None:\n",
    "        nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "    else:\n",
    "        nodes = [n.name for n in graph_def.node]\n",
    "\n",
    "    print(\"nodes\", nodes)\n",
    "\n",
    "# https://www.dlology.com/blog/how-to-convert-trained-keras-model-to-tensorflow-and-make-prediction/\n",
    "def freeze_session(session,\n",
    "                   keep_var_names=None,\n",
    "                   output_names=None,\n",
    "                   clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import (\n",
    "        convert_variables_to_constants,\n",
    "        remove_training_nodes,\n",
    "    )\n",
    "\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(\n",
    "            set(v.op.name for v in tf.global_variables()).difference(\n",
    "                keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        print_nodes(input_graph_def)\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        frozen_graph = remove_training_nodes(frozen_graph)\n",
    "        return frozen_graph\n",
    "\n",
    "def save_model(sess, output_names, directory, filename):\n",
    "    frozen_graph = freeze_session(sess, output_names=output_names)\n",
    "    print_nodes(frozen_graph)\n",
    "    tf.io.write_graph(frozen_graph, directory, filename + \".pb\", as_text=False)\n",
    "    print(\"Model saved to: %s\" % filename + \".pb\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train, x_test, y_test) = load_mnist()\n",
    "plot_4_by_4_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The cell below includes the cryptonets model. It is composed of the following:\n",
    "1. A convolutional layer , with 5 filters of size 5x5, same padding and stride 2. $n^{[l]} = \\dfrac{(n^{[l-1]} + 2 * p^{[l]} - f^{[l]})}{s^{[l]}} + 1$.\n",
    " - Input: 28 x 28 x 1\n",
    " - Output: 14 x 14 x 5\n",
    "2. A square activation function $g(x) = x^2$\n",
    "3. An average pooling layer with 1 stride and same padding. $n^{[l]} = \\dfrac{(n^{[l-1]} - f^{[l]})}{s^{[l]}} + 1$.\n",
    " - Input: 14 x 14 x 5\n",
    " - Output: 14 x 14 x5\n",
    "4. Another convolutional layer , with 50 filters of size 5x5, same padding and stride 2. $n^{[l]} = \\dfrac{(n^{[l-1]} + 2 * p^{[l]} - f^{[l]})}{s^{[l]}} + 1$.\n",
    " - Input: 14 x 14 x 5\n",
    " - Output: 7 x 7 x 50\n",
    "5. Another average pooling layer.\n",
    " - Input: 7 x 7 x 50\n",
    " - Output: 7 x 7 x 50\n",
    "6. A flattening layer, thus resulting in 7 x 7 x 50 = 2450 input attributes to the dense layer\n",
    "7. A dense layer whose output is 100 from 2450\n",
    "8. Another square activation layer\n",
    "9. A final dense layer without any activation after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cryptonets_model(input):\n",
    "\n",
    "    def square_activation(x):\n",
    "        return x * x\n",
    "\n",
    "    y = Conv2D(\n",
    "        filters=5,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=True,\n",
    "        input_shape=(28, 28, 1),\n",
    "        name=\"conv2d_1\",\n",
    "    )(input)\n",
    "    y = Activation(square_activation)(y)\n",
    "\n",
    "    y = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(y)\n",
    "    y = Conv2D(\n",
    "        filters=50,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=True,\n",
    "        name=\"conv2d_2\",\n",
    "    )(y)\n",
    "    y = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(100, use_bias=True, name=\"fc_1\")(y)\n",
    "    y = Activation(square_activation)(y)\n",
    "    y = Dense(10, use_bias=True, name=\"fc_2\")(y)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "The model is created in a \"tensorflow-like\" way. We use the Keras layers to pass through them a placeholder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 5)         130       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 50)          6300      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "fc_1 (Dense)                 (None, 100)               245100    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc_2 (Dense)                 (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 252,540\n",
      "Trainable params: 252,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(28, 28, 1), name=\"input\")\n",
    "y = cryptonets_model(x)\n",
    "cryptonets_model = Model(inputs=x, outputs=y)\n",
    "print(cryptonets_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We first define the loss and the optimizer that we are going to make use of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "optimizer = SGD(learning_rate=0.008, momentum=0.9)\n",
    "cryptonets_model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.5756 - acc: 0.8085 - val_loss: 0.1286 - val_acc: 0.9612\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.1221 - acc: 0.9633 - val_loss: 0.0726 - val_acc: 0.9773\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 8s 137us/sample - loss: 0.0888 - acc: 0.9731 - val_loss: 0.0522 - val_acc: 0.9824\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0703 - acc: 0.9788 - val_loss: 0.0492 - val_acc: 0.9851\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0606 - acc: 0.9814 - val_loss: 0.0490 - val_acc: 0.9853\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0519 - acc: 0.9840 - val_loss: 0.0609 - val_acc: 0.9821\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0482 - acc: 0.9851 - val_loss: 0.0547 - val_acc: 0.9837\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0449 - acc: 0.9861 - val_loss: 0.0391 - val_acc: 0.9880\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0406 - acc: 0.9874 - val_loss: 0.0556 - val_acc: 0.9827\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0386 - acc: 0.9880 - val_loss: 0.0347 - val_acc: 0.9890\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0347 - acc: 0.9893 - val_loss: 0.0500 - val_acc: 0.9851\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0323 - acc: 0.9897 - val_loss: 0.0464 - val_acc: 0.9862\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0314 - acc: 0.9900 - val_loss: 0.0396 - val_acc: 0.9886\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0285 - acc: 0.9911 - val_loss: 0.0469 - val_acc: 0.9871\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0279 - acc: 0.9912 - val_loss: 0.0431 - val_acc: 0.9882\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0274 - acc: 0.9914 - val_loss: 0.0380 - val_acc: 0.9881\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0246 - acc: 0.9925 - val_loss: 0.0378 - val_acc: 0.9899\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0264 - acc: 0.9923 - val_loss: 0.0416 - val_acc: 0.9875\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0239 - acc: 0.9921 - val_loss: 0.0444 - val_acc: 0.9883\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0237 - acc: 0.9919 - val_loss: 0.0420 - val_acc: 0.9875\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0515 - val_acc: 0.9851\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0217 - acc: 0.9927 - val_loss: 0.0437 - val_acc: 0.9868\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0462 - val_acc: 0.9882\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0205 - acc: 0.9933 - val_loss: 0.0497 - val_acc: 0.9858\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0174 - acc: 0.9944 - val_loss: 0.0487 - val_acc: 0.9885\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0191 - acc: 0.9933 - val_loss: 0.0514 - val_acc: 0.9866\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0426 - val_acc: 0.9895\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0157 - acc: 0.9946 - val_loss: 0.0487 - val_acc: 0.9880\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0509 - val_acc: 0.9873\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0163 - acc: 0.9949 - val_loss: 0.0420 - val_acc: 0.9879\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0550 - val_acc: 0.9856\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0150 - acc: 0.9950 - val_loss: 0.0440 - val_acc: 0.9901\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0154 - acc: 0.9948 - val_loss: 0.0464 - val_acc: 0.9880\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0709 - val_acc: 0.9854\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0139 - acc: 0.9956 - val_loss: 0.0428 - val_acc: 0.9890\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0490 - val_acc: 0.9888\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0531 - val_acc: 0.9877\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0554 - val_acc: 0.9878\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.0682 - val_acc: 0.9865\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0482 - val_acc: 0.9883\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0498 - val_acc: 0.9875\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0138 - acc: 0.9957 - val_loss: 0.0456 - val_acc: 0.9895\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0596 - val_acc: 0.9875\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0541 - val_acc: 0.9883\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0553 - val_acc: 0.9878\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0460 - val_acc: 0.9885\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0513 - val_acc: 0.9889\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0537 - val_acc: 0.9884\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0604 - val_acc: 0.9886\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0098 - acc: 0.9973 - val_loss: 0.0543 - val_acc: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1fc55620b8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cryptonets_model.fit(x_train, y_train, \n",
    "                     epochs=50, batch_size=64, \n",
    "                     validation_data=(x_test, y_test), \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptonets_model.save('models/cryptonets_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 123us/sample - loss: 0.0543 - acc: 0.9877\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = cryptonets_model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Squash\n",
    "After having trained the model, we have the weights of the different layers. However, as you may have realised, we make use only of two activation functions in the whole model. The operations inbetween the matrix multiplications can be compressed into a single matrix. The objective of this operation is reducing the number of multiplications that we make on the resulting Neural Network, since it directly influences the noise in the Homomorphic Encryption domain.\n",
    "\n",
    "This process is done specifically for each network model. The reason behind. It is hardly generalizable.\n",
    "\n",
    "In order to extract the bias ($\\vec{b}$) of that operation, the neural network is passed a vector of $0$. $W^T * x + \\vec{b} = \\vec{b}$ if $x = \\vec{0}$\n",
    "\n",
    "Once we have the bias ($\\vec{b}$), we can extract the weights by multiplying by the identity matrix ($I$), and substracting the bias from the result. $W = W*I + b - b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squash linear layers and return squashed weights\n",
    "def squash_layers(cryptonets_model, sess):\n",
    "    # We get the different layers for the model\n",
    "    layers = cryptonets_model.layers\n",
    "    layer_names = [layer.name for layer in layers]\n",
    "    conv1_weights = layers[layer_names.index('conv2d_1')].get_weights()\n",
    "    conv2_weights = layers[layer_names.index('conv2d_2')].get_weights()\n",
    "    fc1_weights = layers[layer_names.index('fc_1')].get_weights()\n",
    "    fc2_weights = layers[layer_names.index('fc_2')].get_weights()\n",
    "\n",
    "    # Get weight into the different layers\n",
    "    y = Input(shape=(14 * 14 * 5,), name=\"squashed_input\")\n",
    "    y = Reshape((14, 14, 5))(y)\n",
    "    y = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(y)\n",
    "    y = Conv2D(\n",
    "        filters=50,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=True,\n",
    "        trainable=False,\n",
    "        kernel_initializer=tf.compat.v1.constant_initializer(conv2_weights[0]),\n",
    "        bias_initializer=tf.compat.v1.constant_initializer(conv2_weights[1]),\n",
    "        name=\"conv2d_test\",\n",
    "    )(y)\n",
    "    y = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(\n",
    "        100,\n",
    "        use_bias=True,\n",
    "        name=\"fc_1\",\n",
    "        kernel_initializer=tf.compat.v1.constant_initializer(fc1_weights[0]),\n",
    "        bias_initializer=tf.compat.v1.constant_initializer(fc1_weights[1]))(y)\n",
    "    \n",
    "    # We initialize the session global variables for the model.\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Pass 0 to get bias. If we multiply by 0, we get 0\n",
    "    squashed_bias = y.eval(\n",
    "        session=sess,\n",
    "        feed_dict={\n",
    "            \"squashed_input:0\": np.zeros((1, 14 * 14 * 5))\n",
    "        })\n",
    "    print(squashed_bias)\n",
    "    \n",
    "    # We execute the computation on the eye/identity matrix\n",
    "    squashed_bias_plus_weights = y.eval(\n",
    "        session=sess, feed_dict={\n",
    "            \"squashed_input:0\": np.eye(14 * 14 * 5)\n",
    "        })\n",
    "    \n",
    "    # If we subtract the bias, we can get the weights\n",
    "    squashed_weights = squashed_bias_plus_weights - squashed_bias\n",
    "    print(squashed_bias)\n",
    "    print(\"squashed layers\")\n",
    "\n",
    "    # Sanity check - Checking that the variation on the result is not enormous\n",
    "    x_in = np.random.rand(100, 14 * 14 * 5)\n",
    "    network_out = y.eval(session=sess, feed_dict={\"squashed_input:0\": x_in})\n",
    "    linear_out = x_in.dot(squashed_weights) + squashed_bias\n",
    "    assert np.max(np.abs(linear_out - network_out)) < 1e-3\n",
    "\n",
    "    return (conv1_weights, (squashed_weights, squashed_bias), fc1_weights,\n",
    "            fc2_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define our new model, where we are making use of the different weights that we extracted before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cryptonets_model_squashed(input, conv1_weights, squashed_weights,\n",
    "                              fc2_weights):\n",
    "\n",
    "    def square_activation(x):\n",
    "        return x * x\n",
    "\n",
    "    y = Conv2D(\n",
    "        filters=5,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=tf.compat.v1.constant_initializer(conv1_weights[0]),\n",
    "        bias_initializer=tf.compat.v1.constant_initializer(conv1_weights[1]),\n",
    "        input_shape=(28, 28, 1),\n",
    "        trainable=False,\n",
    "        name=\"convd1_1\",\n",
    "    )(input)\n",
    "    y = Activation(square_activation)(y)\n",
    "\n",
    "    # Using Keras model API with Flatten results in split ngraph at Flatten() or Reshape() op.\n",
    "    # Use tf.reshape instead\n",
    "    y = tf.reshape(y, [-1, 5 * 14 * 14])\n",
    "    y = Dense(\n",
    "        100,\n",
    "        use_bias=True,\n",
    "        name=\"squash_fc_1\",\n",
    "        trainable=False,\n",
    "        kernel_initializer=tf.compat.v1.constant_initializer(\n",
    "            squashed_weights[0]),\n",
    "        bias_initializer=tf.compat.v1.constant_initializer(squashed_weights[1]),\n",
    "    )(y)\n",
    "    y = Activation(square_activation)(y)\n",
    "\n",
    "    y = Dense(\n",
    "        10,\n",
    "        use_bias=True,\n",
    "        trainable=False,\n",
    "        kernel_initializer=tf.compat.v1.constant_initializer(fc2_weights[0]),\n",
    "        bias_initializer=tf.compat.v1.constant_initializer(fc2_weights[1]),\n",
    "        name=\"output\",\n",
    "    )(y)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Reshape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-589fd9599007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Squash weights and save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m weights = squash_layers(cryptonets_model,\n\u001b[0;32m----> 3\u001b[0;31m                         tf.compat.v1.keras.backend.get_session())\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv1_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquashed_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc2_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-5ce6ef24f2ad>\u001b[0m in \u001b[0;36msquash_layers\u001b[0;34m(cryptonets_model, sess)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get weight into the different layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"squashed_input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     y = Conv2D(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Reshape' is not defined"
     ]
    }
   ],
   "source": [
    "# Squash weights and save model\n",
    "weights = squash_layers(cryptonets_model,\n",
    "                        tf.compat.v1.keras.backend.get_session())\n",
    "(conv1_weights, squashed_weights, fc1_weights, fc2_weights) = weights[0:4]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "x = Input(shape=(28, 28, 1), name=\"input\")\n",
    "y = model.cryptonets_model_squashed(x, conv1_weights, squashed_weights, c2_weights)\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "mnist_util.save_model( sess, [\"output/BiasAdd\"], \"./models\", \"cryptonets\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
